{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khanakshah27/GuardianAI/blob/model/vidcr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UMRtNI6gI5y",
        "outputId": "b4ce57b5-3104-4dfd-f2f9-f0d9636acb40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install torch torchvision opencv-python-headless\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFlKv301YCpJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DEu6KL7Y1s-",
        "outputId": "c37f7cd4-45c8-4432-8590-2b73c7af4d41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/r3d_18-b3b3357e.pth\" to /root/.cache/torch/hub/checkpoints/r3d_18-b3b3357e.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 127M/127M [00:01<00:00, 131MB/s]\n"
          ]
        }
      ],
      "source": [
        "def video_to_embeddings(video_path, model, mean, std, clip_len=16, stride=8, size=112):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    embeddings = []\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frame = cv2.resize(frame, (size, size))\n",
        "        frame = frame.astype(np.float32) / 255.0\n",
        "        frame = (frame - mean) / std\n",
        "        frames.append(frame)\n",
        "    cap.release()\n",
        "\n",
        "    for start in range(0, len(frames) - clip_len + 1, stride):\n",
        "        clip = np.stack(frames[start:start+clip_len], axis=0)\n",
        "        clip = torch.tensor(clip, dtype=torch.float32).permute(3, 0, 1, 2).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            feats = model.stem(clip)\n",
        "            feats = model.layer1(feats)\n",
        "            feats = model.layer2(feats)\n",
        "            feats = model.layer3(feats)\n",
        "            feats = model.layer4(feats)\n",
        "            emb = feats.mean([-3, -2, -1]).squeeze(0).cpu().numpy()\n",
        "            embeddings.append(emb)\n",
        "    return np.stack(embeddings) if embeddings else None\n",
        "\n",
        "mean = np.array([0.43216, 0.394666, 0.37645])\n",
        "std = np.array([0.22803, 0.22145, 0.216989])\n",
        "\n",
        "r3d = r3d_18(weights=R3D_18_Weights.DEFAULT)\n",
        "r3d.fc = nn.Identity()\n",
        "r3d = r3d.eval().to(device).double()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EovI4MdlY4HL"
      },
      "outputs": [],
      "source": [
        "class LSTMAutoEncoder(nn.Module):\n",
        "\n",
        " def __init__(self, input_dim, hidden_dim=128):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    self.decoder = nn.LSTM(hidden_dim, input_dim, batch_first=True)\n",
        "\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        " def forward(self, x):\n",
        "\n",
        "\n",
        "\n",
        "    _, (h, _) = self.encoder(x)\n",
        "\n",
        "    z = h[-1].unsqueeze(0).repeat(x.size(1), 1, 1).permute(1, 0, 2)\n",
        "\n",
        "    decoded, _ = self.decoder(z)\n",
        "\n",
        "    return decoded\n",
        "\n",
        "lstmae = LSTMAutoEncoder(input_dim=512, hidden_dim=128).to(device)\n",
        "lstmae = lstmae.eval()\n",
        "lstmae = lstmae.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUpIWS9ianSn"
      },
      "outputs": [],
      "source": [
        "r3dclassifier = r3d_18(weights=R3D_18_Weights.DEFAULT)\n",
        "r3dclassifier.fc = nn.Linear(r3dclassifier.fc.in_features, 2)\n",
        "r3dclassifier = r3dclassifier.eval().to(device)\n",
        "r3dclassifier = r3dclassifier.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bce1DGombB_A"
      },
      "outputs": [],
      "source": [
        "\n",
        "def fusion_decision(r3d_logits, ae_x, ae_xhat, anomaly_threshold=0.7, crime_conf_threshold=0.75, normal_conf_threshold=0.8): # Lowered crime_conf_threshold\n",
        "\n",
        "    probs = torch.softmax(torch.tensor(r3d_logits), dim=1)\n",
        "    crime_conf = probs[:, 1].max().item()\n",
        "    normal_conf = probs[:, 0].max().item()\n",
        "\n",
        "    ae_x = torch.tensor(ae_x, dtype=torch.float32)\n",
        "    ae_xhat = torch.tensor(ae_xhat, dtype=torch.float32)\n",
        "    mse = torch.mean((ae_x - ae_xhat) ** 2, dim=1)\n",
        "    anomaly_score = mse.mean().item()\n",
        "\n",
        "    print(f\"  Classifier Conf (Crime): {crime_conf:.2f}\")\n",
        "    print(f\"  Classifier Conf (No Crime): {normal_conf:.2f}\")\n",
        "    print(f\"  Anomaly Score: {anomaly_score:.2f}\")\n",
        "\n",
        "    if crime_conf > crime_conf_threshold:\n",
        "        return \"crime\", crime_conf, anomaly_score\n",
        "    elif normal_conf > normal_conf_threshold:\n",
        "        return \"no crime\", normal_conf, anomaly_score\n",
        "    elif anomaly_score > anomaly_threshold:\n",
        "         return \"crime\", crime_conf, anomaly_score\n",
        "    else:\n",
        "        return \"no crime\", normal_conf, anomaly_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPswYvIObC64"
      },
      "outputs": [],
      "source": [
        "\n",
        "embedding_classifier = EmbeddingClassifier(input_dim=512, num_classes=2).to(device)\n",
        "embedding_classifier.load_state_dict(torch.load('embedding_classifier.pth'))\n",
        "embedding_classifier.eval()\n",
        "embedding_classifier = embedding_classifier.float()\n",
        "\n",
        "def analyze_video(video_path):\n",
        "\n",
        "    emb_seq = video_to_embeddings(video_path, r3d, mean, std)\n",
        "    if emb_seq is None:\n",
        "        print(\"No clips found.\")\n",
        "        return\n",
        "\n",
        "    emb_seq_torch = torch.tensor(emb_seq, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        xhat = lstmae(emb_seq_torch).cpu().squeeze(0).numpy()\n",
        "\n",
        "    r3d_preds = []\n",
        "    with torch.no_grad():\n",
        "        clip_embeddings = torch.tensor(emb_seq, dtype=torch.float32).to(device)\n",
        "        logits = embedding_classifier(clip_embeddings)\n",
        "\n",
        "        avg_logits = torch.mean(logits, dim=0).unsqueeze(0).cpu().numpy()\n",
        "        r3d_preds = avg_logits\n",
        "\n",
        "\n",
        "    label, r3d_conf, anom = fusion_decision(r3d_preds, emb_seq, xhat)\n",
        "    print(f\"RESULT: {label.upper()} (Classifier conf={r3d_conf:.2f}, Anomaly score={anom:.2f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5ODajUZVlKv"
      },
      "outputs": [],
      "source": [
        "lstmae = lstmae.to(device).float()\n",
        "r3dclassifier = r3dclassifier.to(device).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjA7FiFFV20Y"
      },
      "outputs": [],
      "source": [
        "for param in r3d.parameters():\n",
        "    param.data = param.data.float()\n",
        "for buf in r3d.buffers():\n",
        "    buf.data = buf.data.float()\n",
        "for param in lstmae.parameters():\n",
        "    param.data = param.data.float()\n",
        "for buf in lstmae.buffers():\n",
        "    buf.data = buf.data.float()\n",
        "for param in r3dclassifier.parameters():\n",
        "    param.data = param.data.float()\n",
        "for buf in r3dclassifier.buffers():\n",
        "    buf.data = buf.data.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZfkbzh-R-to",
        "outputId": "afbb9346-85c2-4474-8e10-8872c3884347"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Classifier Conf (Crime): 0.98\n",
            "  Classifier Conf (No Crime): 0.02\n",
            "  Anomaly Score: 1.05\n",
            "RESULT: CRIME (Classifier conf=0.98, Anomaly score=1.05)\n"
          ]
        }
      ],
      "source": [
        "analyze_video(\"/content/crime5.mp4\")\n",
        "#analyze_video(\"/content/good2.mp4\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKBRaXjfibI7",
        "outputId": "dd9514a4-9aec-4440-b6e1-eb1bb4dcef9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating pipeline accuracy on the testing dataset...\n",
            "\n",
            "Analyzing /content/crime4.mp4 (Ground Truth: crime)\n",
            "Classifier Conf (Crime): 0.95\n",
            "  Classifier Conf (No Crime): 0.05\n",
            "  Anomaly Score: 0.59\n",
            "RESULT: CRIME (Classifier conf=0.95, Anomaly score=0.59)\n",
            "Predicted: crime\n",
            "\n",
            "Analyzing /content/good1.mp4 (Ground Truth: no crime)\n",
            "Classifier Conf (Crime): 0.02\n",
            "  Classifier Conf (No Crime): 0.98\n",
            "  Anomaly Score: 0.87\n",
            "RESULT: NO CRIME (Classifier conf=0.98, Anomaly score=0.87)\n",
            "Predicted: no crime\n",
            "\n",
            "Overall Accuracy on Testing Dataset: 100.00%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "full_dataset = [\n",
        "    (\"/content/crime4.mp4\", \"crime\"),\n",
        "    (\"/content/crime3.mp4\", \"crime\"),\n",
        "    (\"/content/good1.mp4\", \"no crime\"),\n",
        "    (\"/content/crime5.mp4\", \"crime\"),\n",
        "    (\"/content/good2.mp4\", \"no crime\"),\n",
        "    (\"/content/good3.mp4\", \"no crime\"),\n",
        "    (\"/content/good4.mp4\", \"no crime\"),\n",
        "    (\"/content/crime6.mp4\", \"crime\"),\n",
        "\n",
        "]\n",
        "\n",
        "import random\n",
        "\n",
        "random.shuffle(full_dataset)\n",
        "\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "training_dataset = full_dataset[:train_size]\n",
        "testing_dataset = full_dataset[train_size:]\n",
        "\n",
        "\n",
        "correct_predictions = 0\n",
        "total_videos = len(testing_dataset)\n",
        "\n",
        "print(\"Evaluating pipeline accuracy on the testing dataset...\")\n",
        "\n",
        "for video_path, ground_truth in testing_dataset:\n",
        "    print(f\"\\nAnalyzing {video_path} (Ground Truth: {ground_truth})\")\n",
        "\n",
        "    import sys\n",
        "    from io import StringIO\n",
        "    old_stdout = sys.stdout\n",
        "    redirected_output = StringIO()\n",
        "    sys.stdout = redirected_output\n",
        "\n",
        "    try:\n",
        "        analyze_video(video_path)\n",
        "    finally:\n",
        "        sys.stdout = old_stdout\n",
        "\n",
        "    output = redirected_output.getvalue().strip()\n",
        "    print(output)\n",
        "\n",
        "    predicted_label = \"unknown\"\n",
        "    if \"RESULT: CRIME\" in output:\n",
        "        predicted_label = \"crime\"\n",
        "    elif \"RESULT: NO CRIME\" in output:\n",
        "        predicted_label = \"no crime\"\n",
        "\n",
        "    print(f\"Predicted: {predicted_label}\")\n",
        "\n",
        "    if predicted_label == ground_truth:\n",
        "        correct_predictions += 1\n",
        "\n",
        "accuracy = (correct_predictions / total_videos) * 100 if total_videos > 0 else 0\n",
        "print(f\"\\nOverall Accuracy on Testing Dataset: {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "267ae0c1"
      },
      "outputs": [],
      "source": [
        "training_dataset = [\n",
        "    (\"/content/crime4.mp4\", \"crime\"),\n",
        "    (\"/content/crime3.mp4\", \"crime\"),\n",
        "    (\"/content/good1.mp4\", \"no crime\"),\n",
        "    (\"/content/crime5.mp4\", \"crime\"),\n",
        "    (\"/content/good2.mp4\", \"no crime\"),\n",
        "    (\"/content/good3.mp4\", \"no crime\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzigrY0Rm954",
        "outputId": "a83bdfe4-762b-489c-d2a5-ca7ef21d27fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50, Loss: 0.9745\n",
            "Epoch 2/50, Loss: 0.5871\n",
            "Epoch 3/50, Loss: 0.4393\n",
            "Epoch 4/50, Loss: 0.3849\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "class EmbeddingClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "embedding_classifier = EmbeddingClassifier(input_dim=512, num_classes=2).to(device)\n",
        "embedding_classifier = embedding_classifier.float()\n",
        "\n",
        "def train_embedding_classifier(model, feature_extractor, dataset, criterion, optimizer, device, mean, std, epochs=10):\n",
        "    model.train()\n",
        "    feature_extractor.eval()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for video_path, label in dataset:\n",
        "\n",
        "            emb_seq = video_to_embeddings(video_path, feature_extractor, mean, std)\n",
        "            if emb_seq is None:\n",
        "                print(f\"Skipping {video_path}: No clips found.\")\n",
        "                continue\n",
        "\n",
        "            target = torch.tensor([1 if label == \"crime\" else 0], dtype=torch.long).to(device)\n",
        "\n",
        "            clip_embeddings = torch.tensor(emb_seq, dtype=torch.float32).to(device)\n",
        "            logits = model(clip_embeddings)\n",
        "\n",
        "            avg_logits = torch.mean(logits, dim=0).unsqueeze(0)\n",
        "\n",
        "            loss = criterion(avg_logits, target)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(dataset):.4f}\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(embedding_classifier.parameters(), lr=0.001)\n",
        "\n",
        "train_embedding_classifier(embedding_classifier, r3d, training_dataset, criterion, optimizer, device, mean, std, epochs=50)\n",
        "\n",
        "torch.save(embedding_classifier.state_dict(), 'embedding_classifier.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2707241d"
      },
      "source": [
        "## Classifier training\n",
        "\n",
        "### Subtask:\n",
        "Train the R3D classifier on the prepared dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "796344c4"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the training function for the R3D classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "003f7520",
        "outputId": "3180bdda-e69b-4802-fd50-691cc21543cd"
      },
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
        "import os\n",
        "\n",
        "class EmbeddingClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "embedding_classifier = EmbeddingClassifier(input_dim=512, num_classes=2).to(device)\n",
        "embedding_classifier = embedding_classifier.float()\n",
        "def preextract_embeddings(dataset, feature_extractor, device, mean, std, output_dir=\"preextracted_embeddings\"):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    feature_extractor.eval()\n",
        "    for video_path, label in dataset:\n",
        "        video_name = os.path.basename(video_path).split('.')[0]\n",
        "        output_path = os.path.join(output_dir, f\"{video_name}_embeddings.pt\")\n",
        "        if os.path.exists(output_path):\n",
        "            print(f\"Embeddings for {video_name} already exist. Skipping extraction.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Extracting embeddings for {video_path}...\")\n",
        "        emb_seq = video_to_embeddings(video_path, feature_extractor, mean, std)\n",
        "        if emb_seq is not None:\n",
        "\n",
        "            torch.save({'embeddings': emb_seq, 'label': label}, output_path)\n",
        "            print(f\"Saved embeddings for {video_name}\")\n",
        "        else:\n",
        "            print(f\"Could not extract embeddings for {video_path}\")\n",
        "\n",
        "\n",
        "preextract_embeddings(training_dataset, r3d, device, mean, std)\n",
        "\n",
        "def train_embedding_classifier_from_preextracted(model, preextracted_dir, criterion, optimizer, device, epochs=10):\n",
        "    model.train()\n",
        "\n",
        "    preextracted_files = [os.path.join(preextracted_dir, f) for f in os.listdir(preextracted_dir) if f.endswith(\".pt\")]\n",
        "    print(f\"Found {len(preextracted_files)} pre-extracted embedding files.\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for file_path in preextracted_files:\n",
        "\n",
        "            data = torch.load(file_path, weights_only=False)\n",
        "            emb_seq = data['embeddings']\n",
        "            label = data['label']\n",
        "\n",
        "            target = torch.tensor([1 if label == \"crime\" else 0], dtype=torch.long).to(device)\n",
        "\n",
        "            clip_embeddings = torch.tensor(emb_seq, dtype=torch.float32).to(device)\n",
        "            logits = model(clip_embeddings)\n",
        "            avg_logits = torch.mean(logits, dim=0).unsqueeze(0)\n",
        "            loss = criterion(avg_logits, target)\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(preextracted_files):.4f}\")\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(embedding_classifier.parameters(), lr=0.001)\n",
        "\n",
        "train_embedding_classifier_from_preextracted(embedding_classifier, \"preextracted_embeddings\", criterion, optimizer, device, epochs=20) # Reduced epochs\n",
        "\n",
        "torch.save(embedding_classifier.state_dict(), 'embedding_classifier.pth')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings for crime4 already exist. Skipping extraction.\n",
            "Embeddings for crime3 already exist. Skipping extraction.\n",
            "Embeddings for good1 already exist. Skipping extraction.\n",
            "Embeddings for crime5 already exist. Skipping extraction.\n",
            "Embeddings for good2 already exist. Skipping extraction.\n",
            "Embeddings for good3 already exist. Skipping extraction.\n",
            "Found 6 pre-extracted embedding files.\n",
            "Epoch 1/20, Loss: 0.7808\n",
            "Epoch 2/20, Loss: 0.4020\n",
            "Epoch 3/20, Loss: 0.3517\n",
            "Epoch 4/20, Loss: 0.3213\n",
            "Epoch 5/20, Loss: 0.2554\n",
            "Epoch 6/20, Loss: 0.2019\n",
            "Epoch 7/20, Loss: 0.1628\n",
            "Epoch 8/20, Loss: 0.1355\n",
            "Epoch 9/20, Loss: 0.1178\n",
            "Epoch 10/20, Loss: 0.1048\n",
            "Epoch 11/20, Loss: 0.0936\n",
            "Epoch 12/20, Loss: 0.0836\n",
            "Epoch 13/20, Loss: 0.0750\n",
            "Epoch 14/20, Loss: 0.0677\n",
            "Epoch 15/20, Loss: 0.0617\n",
            "Epoch 16/20, Loss: 0.0566\n",
            "Epoch 17/20, Loss: 0.0523\n",
            "Epoch 18/20, Loss: 0.0485\n",
            "Epoch 19/20, Loss: 0.0452\n",
            "Epoch 20/20, Loss: 0.0422\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "briCTdAJL440"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPj0r7NHZz1E2pLFiLEX7ML",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}